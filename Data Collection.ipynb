{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa4019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio[ffmpeg] in d:\\anaconda\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from imageio[ffmpeg]) (1.24.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\anaconda\\lib\\site-packages (from imageio[ffmpeg]) (10.0.1)\n",
      "Requirement already satisfied: imageio-ffmpeg in d:\\anaconda\\lib\\site-packages (from imageio[ffmpeg]) (0.4.9)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\lib\\site-packages (from imageio[ffmpeg]) (5.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (68.0.0)\n",
      "Requirement already satisfied: imageio[pyav] in d:\\anaconda\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from imageio[pyav]) (1.24.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\anaconda\\lib\\site-packages (from imageio[pyav]) (10.0.1)\n",
      "Requirement already satisfied: av in d:\\anaconda\\lib\\site-packages (from imageio[pyav]) (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio[ffmpeg]\n",
    "!pip install imageio[pyav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f6826a-ef26-4e55-90ff-ee2fbdc5c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "import json\n",
    "import statistics\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import deque\n",
    "from constants import TOTAL_FRAMES, VALID_WORD_THRESHOLD, NOT_TALKING_THRESHOLD, PAST_BUFFER_SIZE, LIP_WIDTH, LIP_HEIGHT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be2a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What word you like to collect data for? The options are \n",
      "here, is, a, demo, can, you, read, my, lips, cat, dog, hello, bye: hello\n",
      "If you want, enter a custom lip distance threshold or -1: -1\n",
      "To clean output directory of the current word, type 'yes': yes\n"
     ]
    }
   ],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"face_weights.dat\")\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "curr_word_frames = []\n",
    "not_talking_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "data_count = 1\n",
    "words = [\"here\", \"is\", \"a\", \"demo\", \"can\", \"you\", \"read\", \"my\", \"lips\", \"cat\", \"dog\", \"hello\", \"bye\"]\n",
    "options = \", \".join(words)\n",
    "label = input(\"What word you like to collect data for? The options are \\n\" + options + \": \")\n",
    "labels = []\n",
    "\n",
    "custom_distance = input(\"If you want, enter a custom lip distance threshold or -1: \")\n",
    "\n",
    "clean_output_dir = input(\"To clean output directory of the current word, type 'yes': \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3589354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_output_dir == \"yes\":\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    outputs_dir = os.path.join(root_dir, \"outputs\")\n",
    "    for folder_name in os.listdir(outputs_dir):\n",
    "        folder_path = os.path.join(outputs_dir, folder_name)\n",
    "        if os.path.isdir(folder_path) and label in folder_path:\n",
    "            print(f\"Removing folder {folder_name}...\")\n",
    "            os.system(f\"rm -rf {folder_path}\")\n",
    "\n",
    "\n",
    "past_word_frames = deque(maxlen=PAST_BUFFER_SIZE)\n",
    "\n",
    "\n",
    "determining_lip_distance = 50\n",
    "\n",
    "\n",
    "lip_distances = []\n",
    "\n",
    "LIP_DISTANCE_THRESHOLD = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf24b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_distance != -1 and custom_distance.isdigit() and int(custom_distance) > 0:\n",
    "    custom_distance = int(custom_distance)\n",
    "    determining_lip_distance = 0\n",
    "    LIP_DISTANCE_THRESHOLD = custom_distance\n",
    "    print(\"USING CUSTOM DISTANCE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6a8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        x1 = face.left()\n",
    "        y1 = face.top()  \n",
    "        x2 = face.right()  \n",
    "        y2 = face.bottom() \n",
    "        landmarks = predictor(image=gray, box=face)\n",
    "        mouth_top = (landmarks.part(51).x, landmarks.part(51).y)\n",
    "        mouth_bottom = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "        lip_distance = math.hypot(mouth_bottom[0] - mouth_top[0], mouth_bottom[1] - mouth_top[1])\n",
    "        lip_left = landmarks.part(48).x\n",
    "        lip_right = landmarks.part(54).x\n",
    "        lip_top = landmarks.part(50).y\n",
    "        lip_bottom = landmarks.part(58).y\n",
    "        if(determining_lip_distance != 0 and LIP_DISTANCE_THRESHOLD != None):\n",
    "            width_diff = LIP_WIDTH - (lip_right - lip_left)\n",
    "            height_diff = LIP_HEIGHT - (lip_bottom - lip_top)\n",
    "            pad_left = width_diff // 2\n",
    "            pad_right = width_diff - pad_left\n",
    "            pad_top = height_diff // 2\n",
    "            pad_bottom = height_diff - pad_top\n",
    "            pad_left = min(pad_left, lip_left)\n",
    "            pad_right = min(pad_right, frame.shape[1] - lip_right)\n",
    "            pad_top = min(pad_top, lip_top)\n",
    "            pad_bottom = min(pad_bottom, frame.shape[0] - lip_bottom)\n",
    "            lip_frame = frame[lip_top - pad_top:lip_bottom + pad_bottom, lip_left - pad_left:lip_right + pad_right]\n",
    "            lip_frame = cv2.resize(lip_frame, (LIP_WIDTH, LIP_HEIGHT))           \n",
    "            lip_frame_lab = cv2.cvtColor(lip_frame, cv2.COLOR_BGR2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lip_frame_lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(3,3))\n",
    "            l_channel_eq = clahe.apply(l_channel)\n",
    "            lip_frame_eq = cv2.merge((l_channel_eq, a_channel, b_channel))\n",
    "            lip_frame_eq = cv2.cvtColor(lip_frame_eq, cv2.COLOR_LAB2BGR)\n",
    "            lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (7, 7), 0)\n",
    "            lip_frame_eq = cv2.bilateralFilter(lip_frame_eq, 5, 75, 75)\n",
    "            kernel = np.array([[-1,-1,-1],\n",
    "                       [-1, 9,-1],\n",
    "                       [-1,-1,-1]])\n",
    "            lip_frame_eq = cv2.filter2D(lip_frame_eq, -1, kernel)\n",
    "            lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (5, 5), 0)\n",
    "            lip_frame = lip_frame_eq\n",
    "            for n in range(48, 61):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 255, 0), thickness=-1)\n",
    "            ORANGE =  (0, 180, 255)\n",
    "            BLUE = (255, 0, 0)\n",
    "            RED = (0, 0, 255) \n",
    "            if lip_distance > LIP_DISTANCE_THRESHOLD: # person is talking\n",
    "                cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                curr_word_frames += [lip_frame.tolist()]\n",
    "                not_talking_counter = 0\n",
    "\n",
    "                cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Not talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, RED, 2)\n",
    "                not_talking_counter += 1\n",
    "                if not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE == TOTAL_FRAMES: \n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "\n",
    "                    data_count += 1\n",
    "                    curr_word_frames = list(past_word_frames) + curr_word_frames\n",
    "                    print(f\"adding {label.upper()} shape\", lip_frame.shape, \"count is\", data_count, \"frames is\", len(curr_word_frames))\n",
    "\n",
    "                    all_words.append(curr_word_frames)\n",
    "                    labels.append(label)\n",
    "                    curr_word_frames = []\n",
    "                    not_talking_counter = 0\n",
    "                elif not_talking_counter < NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE < TOTAL_FRAMES and len(curr_word_frames) > VALID_WORD_THRESHOLD:\n",
    "                    cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "                    curr_word_frames += [lip_frame.tolist()]\n",
    "                    not_talking_counter = 0\n",
    "                elif len(curr_word_frames) < VALID_WORD_THRESHOLD or (not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE > TOTAL_FRAMES):\n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "                    curr_word_frames = []\n",
    "                elif not_talking_counter < NOT_TALKING_THRESHOLD:\n",
    "                    cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "                past_word_frames+= [lip_frame.tolist()]\n",
    "                if len(past_word_frames) > PAST_BUFFER_SIZE:\n",
    "                    past_word_frames.pop(0)\n",
    "        else:\n",
    "            cv2.putText(frame, \"KEEP MOUTH CLOSED, CALIBRATING DISTANCE BETWEEN LIPS\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            determining_lip_distance -= 1\n",
    "            distance = landmarks.part(58).y - landmarks.part(50).y \n",
    "            cv2.putText(frame, \"Current distance: \" + str(distance + 2), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            lip_distances.append(distance)\n",
    "            if(determining_lip_distance == 0):\n",
    "                LIP_DISTANCE_THRESHOLD = sum(lip_distances) / len(lip_distances) + 2\n",
    "    cv2.putText(frame, \"COLLECTED WORDS: \" + str(len(all_words)), (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "    cv2.putText(frame, \"Press 'ESC' to exit\", (900, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow(winname=\"Mouth\", mat=frame)\n",
    "    if cv2.waitKey(delay=1) == 27:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995a14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(all_words, labels):\n",
    "    median_length = statistics.median([len(sublist) for sublist in all_words])\n",
    "    median_length = int(median_length)\n",
    "    print(\"Removing sublists shorter than the median length\")\n",
    "    indices_to_keep = [i for i, sublist in enumerate(all_words) if (len(sublist) >= median_length and  len(sublist) <= median_length + 2)]\n",
    "    all_words = [all_words[i] for i in indices_to_keep]\n",
    "    labels = [labels[i] for i in indices_to_keep]\n",
    "    all_words = [sublist[:median_length] for sublist in all_words]\n",
    "    return all_words, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69782fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving words into dir!\n"
     ]
    }
   ],
   "source": [
    "def saveAllWords(all_words):\n",
    "\n",
    "    print(\"saving words into dir!\")\n",
    "    output_dir = \"../collected_data\"\n",
    "    next_dir_number = 1\n",
    "    for i, word_frames in enumerate(all_words):\n",
    "\n",
    "        label = labels[i]\n",
    "\n",
    "        word_folder = os.path.join(output_dir, label + \"_\" + f\"{next_dir_number}\")\n",
    "        while os.path.exists(word_folder):\n",
    "            next_dir_number += 1\n",
    "            word_folder = os.path.join(output_dir, label + \"_\" + f\"{next_dir_number}\")\n",
    "        \n",
    "        os.makedirs(word_folder)\n",
    "\n",
    "        txt_path = os.path.join(word_folder, \"data.txt\")\n",
    "\n",
    "        with open(txt_path, \"w\") as f:\n",
    "            f.write(json.dumps(word_frames))\n",
    "\n",
    "        images = []\n",
    "\n",
    "        for j, img_data in enumerate(word_frames):\n",
    "            img = Image.new('RGB', (len(img_data[0]), len(img_data)))\n",
    "            pixels = img.load()\n",
    "            for y in range(len(img_data)):\n",
    "                for x in range(len(img_data[y])):\n",
    "                    pixels[x, y] = tuple(img_data[y][x])\n",
    "            img_path = os.path.join(word_folder, f\"{j}.png\")\n",
    "            img.save(img_path)\n",
    "            images.append(imageio.imread(img_path))\n",
    "        print(\"The length of this subfolder:\", len(images))\n",
    "        video_path = os.path.join(word_folder, \"video.mp4\")\n",
    "        imageio.mimsave(video_path, images, fps=int(cap.get(cv2.CAP_PROP_FPS)))\n",
    "        next_dir_number += 1\n",
    "\n",
    "saveAllWords(all_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4228d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
