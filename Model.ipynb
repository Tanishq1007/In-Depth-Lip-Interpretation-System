{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe64480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "import json\n",
    "import statistics\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../data')\n",
    "from constants import *\n",
    "from constants import TOTAL_FRAMES, VALID_WORD_THRESHOLD, NOT_TALKING_THRESHOLD, PAST_BUFFER_SIZE, LIP_WIDTH, LIP_HEIGHT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb28a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {6: 'hello', 5: 'dog', 10: 'my', 12: 'you', 9: 'lips', 3: 'cat', 11: 'read', 0: 'a', 4: 'demo', 7: 'here', 8: 'is', 1: 'bye', 2: 'can'}\n",
    "count = 0\n",
    "\n",
    "input_shape = (TOTAL_FRAMES, 80, 112, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9e3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(16, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(len(label_dict), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded3653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(16, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(label_dict), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b8850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5', by_name=True)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "predictor = dlib.shape_predictor(\"face_weights.dat\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "curr_word_frames = []\n",
    "not_talking_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e101a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = True\n",
    "labels = []\n",
    "\n",
    "past_word_frames = deque(maxlen=PAST_BUFFER_SIZE)\n",
    "\n",
    "ending_buffer_size = 5\n",
    "\n",
    "predicted_word_label = None\n",
    "draw_prediction = False\n",
    "\n",
    "spoken_already = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563e62bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* (1, 22, 80, 112, 3)\n",
      "[]\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "demo: 0.869\n",
      "hello: 0.131\n",
      "is: 0.000\n",
      "can: 0.000\n",
      "you: 0.000\n",
      "here: 0.000\n",
      "bye: 0.000\n",
      "read: 0.000\n",
      "a: 0.000\n",
      "lips: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "FINISHED! demo\n",
      "********* (1, 22, 80, 112, 3)\n",
      "['demo']\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "demo: 1.000\n",
      "hello: 0.000\n",
      "is: 0.000\n",
      "can: 0.000\n",
      "you: 0.000\n",
      "bye: 0.000\n",
      "a: 0.000\n",
      "lips: 0.000\n",
      "here: 0.000\n",
      "read: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "FINISHED! hello\n",
      "********* (1, 22, 80, 112, 3)\n",
      "['demo', 'hello']\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "is: 0.984\n",
      "hello: 0.016\n",
      "can: 0.000\n",
      "demo: 0.000\n",
      "you: 0.000\n",
      "here: 0.000\n",
      "lips: 0.000\n",
      "bye: 0.000\n",
      "a: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "read: 0.000\n",
      "FINISHED! is\n",
      "********* (1, 22, 80, 112, 3)\n",
      "['demo', 'hello', 'is']\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "hello: 0.797\n",
      "is: 0.203\n",
      "can: 0.000\n",
      "demo: 0.000\n",
      "you: 0.000\n",
      "here: 0.000\n",
      "lips: 0.000\n",
      "read: 0.000\n",
      "bye: 0.000\n",
      "a: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "FINISHED! can\n",
      "********* (1, 22, 80, 112, 3)\n",
      "['demo', 'hello', 'is', 'can']\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "hello: 1.000\n",
      "demo: 0.000\n",
      "is: 0.000\n",
      "can: 0.000\n",
      "you: 0.000\n",
      "here: 0.000\n",
      "lips: 0.000\n",
      "bye: 0.000\n",
      "read: 0.000\n",
      "a: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "FINISHED! you\n",
      "********* (1, 22, 80, 112, 3)\n",
      "['demo', 'hello', 'is', 'can', 'you']\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "is: 1.000\n",
      "hello: 0.000\n",
      "can: 0.000\n",
      "demo: 0.000\n",
      "you: 0.000\n",
      "lips: 0.000\n",
      "here: 0.000\n",
      "read: 0.000\n",
      "bye: 0.000\n",
      "a: 0.000\n",
      "cat: 0.000\n",
      "dog: 0.000\n",
      "my: 0.000\n",
      "FINISHED! lips\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    "    faces = detector(gray)\n",
    "    \n",
    "    for face in faces:\n",
    "        x1 = face.left()  \n",
    "        y1 = face.top()  \n",
    "        x2 = face.right()  \n",
    "        y2 = face.bottom() \n",
    "\n",
    "        \n",
    "        landmarks = predictor(image=gray, box=face)\n",
    "\n",
    "        mouth_top = (landmarks.part(51).x, landmarks.part(51).y)\n",
    "        mouth_bottom = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "        lip_distance = math.hypot(mouth_bottom[0] - mouth_top[0], mouth_bottom[1] - mouth_top[1])\n",
    "\n",
    "\n",
    "\n",
    "        lip_left = landmarks.part(48).x\n",
    "        lip_right = landmarks.part(54).x\n",
    "        lip_top = landmarks.part(50).y\n",
    "        lip_bottom = landmarks.part(58).y\n",
    "\n",
    "       \n",
    "        width_diff = LIP_WIDTH - (lip_right - lip_left)\n",
    "        height_diff = LIP_HEIGHT - (lip_bottom - lip_top)\n",
    "        pad_left = width_diff // 2\n",
    "        pad_right = width_diff - pad_left\n",
    "        pad_top = height_diff // 2\n",
    "        pad_bottom = height_diff - pad_top\n",
    "\n",
    "    \n",
    "        pad_left = min(pad_left, lip_left)\n",
    "        pad_right = min(pad_right, frame.shape[1] - lip_right)\n",
    "        pad_top = min(pad_top, lip_top)\n",
    "        pad_bottom = min(pad_bottom, frame.shape[0] - lip_bottom)\n",
    "\n",
    "      \n",
    "        lip_frame = frame[lip_top - pad_top:lip_bottom + pad_bottom, lip_left - pad_left:lip_right + pad_right]\n",
    "        lip_frame = cv2.resize(lip_frame, (LIP_WIDTH, LIP_HEIGHT))\n",
    "\n",
    "        \n",
    "        lip_frame_lab = cv2.cvtColor(lip_frame, cv2.COLOR_BGR2LAB)\n",
    "       \n",
    "        l_channel, a_channel, b_channel = cv2.split(lip_frame_lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(3,3))\n",
    "        l_channel_eq = clahe.apply(l_channel)\n",
    "\n",
    "       \n",
    "        lip_frame_eq = cv2.merge((l_channel_eq, a_channel, b_channel))\n",
    "        lip_frame_eq = cv2.cvtColor(lip_frame_eq, cv2.COLOR_LAB2BGR)\n",
    "        lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (7, 7), 0)\n",
    "        lip_frame_eq = cv2.bilateralFilter(lip_frame_eq, 5, 75, 75)\n",
    "        kernel = np.array([[-1,-1,-1],\n",
    "                   [-1, 9,-1],\n",
    "                   [-1,-1,-1]])\n",
    "\n",
    "        \n",
    "        lip_frame_eq = cv2.filter2D(lip_frame_eq, -1, kernel)\n",
    "        lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (5, 5), 0)\n",
    "        lip_frame = lip_frame_eq\n",
    "        \n",
    "        \n",
    "        for n in range(48, 61):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "        if lip_distance > 45:\n",
    "            cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            curr_word_frames += [lip_frame.tolist()]\n",
    "        \n",
    "            not_talking_counter = 0\n",
    "            draw_prediction = False\n",
    "        else:\n",
    "            cv2.putText(frame, \"Not talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            not_talking_counter += 1\n",
    "            if not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE == TOTAL_FRAMES: \n",
    "\n",
    "                curr_word_frames = list(past_word_frames) + curr_word_frames\n",
    "\n",
    "                curr_data = np.array([curr_word_frames[:input_shape[0]]])\n",
    "\n",
    "                print(\"*********\", curr_data.shape)\n",
    "                print(spoken_already)\n",
    "                prediction = model.predict(curr_data)\n",
    "\n",
    "                prob_per_class = []\n",
    "                for i in range(len(prediction[0])):\n",
    "                    prob_per_class.append((prediction[0][i], label_dict[i]))\n",
    "                sorted_probs = sorted(prob_per_class, key=lambda x: x[0], reverse=True)\n",
    "                for prob, label in sorted_probs:\n",
    "                    print(f\"{label}: {prob:.3f}\")\n",
    "\n",
    "                predicted_class_index = np.argmax(prediction)\n",
    "                while label_dict[predicted_class_index] in spoken_already:\n",
    "                    \n",
    "                    prediction[0][predicted_class_index] = 0\n",
    "                    predicted_class_index = np.argmax(prediction)\n",
    "                predicted_word_label = label_dict[predicted_class_index]\n",
    "                spoken_already.append(predicted_word_label)\n",
    "\n",
    "                print(\"FINISHED!\", predicted_word_label)\n",
    "                draw_prediction = True\n",
    "                count = 0\n",
    "\n",
    "                curr_word_frames = []\n",
    "                not_talking_counter = 0\n",
    "            elif not_talking_counter < NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE < TOTAL_FRAMES and len(curr_word_frames) > VALID_WORD_THRESHOLD:\n",
    "                curr_word_frames += [lip_frame.tolist()]\n",
    "                not_talking_counter = 0\n",
    "            elif len(curr_word_frames) < VALID_WORD_THRESHOLD or (not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE > TOTAL_FRAMES):\n",
    "                curr_word_frames = []\n",
    "\n",
    "            past_word_frames+= [lip_frame.tolist()]\n",
    "            if len(past_word_frames) > PAST_BUFFER_SIZE:\n",
    "                past_word_frames.pop(0)\n",
    "\n",
    "    if(draw_prediction and count < 20):\n",
    "        count += 1\n",
    "        cv2.putText(frame, predicted_word_label, (50 ,100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(winname=\"Mouth\", mat=frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        spoken_already = []\n",
    "\n",
    "\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db52b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
